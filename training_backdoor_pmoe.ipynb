{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1750841614805,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "qMkNHM0g5Qke",
    "outputId": "8db0a19a-4c27-4e71-cec7-eb961e9eeb73"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install --upgrade pip\n",
    "# !pip install empatches\n",
    "# !pip install tensorflow\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750841614839,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "kmAdfc8GSnzY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:36:36.800994: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-01 19:36:36.825676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751391396.846208  679051 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751391396.852529  679051 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751391396.869197  679051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751391396.869214  679051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751391396.869216  679051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751391396.869217  679051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-01 19:36:36.875219: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from empatches import EMPatches\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from random import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import traceback\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750841615821,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "yQfrshqK120J"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dataset = 'CIFAR-10' # 'CIFAR-10' , 'GTSRB'\n",
    "trigger = 'square' # 'square' , 'blend' , 'warped'\n",
    "# poisoning_rate = 0.0001  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "# poisoning_rate = 0.0005  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "# poisoning_rate = 0.001  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "# poisoning_rate = 0.005  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "poisoning_rate = 0.02  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "# poisoning_rate = 0.06  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "# poisoning_rate = 0.1  # 2% poisoning rate, adjust this to other poisoning rates\n",
    "\n",
    "_pr = 0.3\n",
    "\n",
    "_warping = 0.25 #normal 2 0.0 for warped patch level, training 0.25\n",
    "weight = 0.5 #normal 0.5 0.78 for blend patch level\n",
    "_size = 4 # 1 for patch level\n",
    "n_experts = 4\n",
    "patch_level = False # False for trigger on entire image\n",
    "\n",
    "train = True\n",
    "save_model = True\n",
    "save_pruned_model = False\n",
    "load_pruned_model = False\n",
    "load_model = False\n",
    "target_label = 0    # Target label for backdoor attack\n",
    "source_label = None    # Source label for backdoor attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-ycU9O-5Qkg"
   },
   "source": [
    "# Gating Routers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1750841615830,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "cHgmRT3H5Qkg"
   },
   "outputs": [],
   "source": [
    "class gate(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, gating_kernel_size, strides=(1,1), padding = 'valid',\n",
    "                 data_format = 'channels_last', gating_activation = None,\n",
    "                 gating_kernel_initializer = tf.keras.initializers.RandomNormal, **kwargs):\n",
    "\n",
    "        super(gate, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.gating_kernel_size = gating_kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.gating_activation = tf.keras.activations.get(gating_activation)\n",
    "        self.gating_kernel_initializer = gating_kernel_initializer\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        gating_kernel_shape = self.gating_kernel_size + (input_dim, 1)\n",
    "        self.gating_kernel = self.add_weight(shape=gating_kernel_shape,\n",
    "                                      initializer=self.gating_kernel_initializer,\n",
    "                                      name='gating_kernel')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        gating_outputs = tf.keras.backend.conv2d(inputs, self.gating_kernel, strides=self.strides,\n",
    "                                  padding=self.padding,data_format=self.data_format)\n",
    "\n",
    "        gating_outputs = tf.transpose(gating_outputs, perm=(0,3,1,2))\n",
    "        x = tf.shape(gating_outputs)[2]\n",
    "        y = tf.shape(gating_outputs)[3]\n",
    "        gating_outputs = tf.reshape(gating_outputs,(tf.shape(gating_outputs)[0],tf.shape(gating_outputs)[1],\n",
    "                                                    x*y))\n",
    "\n",
    "        gating_outputs = self.gating_activation(gating_outputs)\n",
    "        # print(\"gating output: \", gating_outputs.shape)\n",
    "        [values, indices] = tf.math.top_k(gating_outputs,k=self.k, sorted=False)\n",
    "        # print(\"value output: \", values.shape)\n",
    "        # print(\"indice before output: \", indices.shape)\n",
    "        indices = tf.reshape(indices,(tf.shape(indices)[0]*tf.shape(indices)[1],tf.shape(indices)[2]))\n",
    "        # print(\"indice after output: \", indices.shape)\n",
    "        values = tf.reshape(values, (tf.shape(values)[0]*tf.shape(values)[1], tf.shape(values)[2]))\n",
    "        batch_t, k_t = tf.unstack(tf.shape(indices), num=2)\n",
    "\n",
    "        n=tf.shape(gating_outputs)[2]\n",
    "\n",
    "        indices_flat = tf.reshape(indices, [-1]) + tf.math.floordiv(tf.range(batch_t * k_t), k_t) * n\n",
    "        ret_flat = tf.math.unsorted_segment_sum(tf.reshape(values, [-1]), indices_flat, batch_t * n)\n",
    "        ret_rsh=tf.reshape(ret_flat, [batch_t, n])\n",
    "        ret_rsh_3=tf.reshape(ret_rsh,(tf.shape(gating_outputs)[0],tf.shape(gating_outputs)[1],tf.shape(gating_outputs)[2]))\n",
    "\n",
    "        new_gating_outputs = tf.reshape(ret_rsh_3,(tf.shape(ret_rsh_3)[0],tf.shape(ret_rsh_3)[1],x,y))\n",
    "        new_gating_outputs = tf.transpose(new_gating_outputs, perm=(0,2,3,1))\n",
    "        new_gating_outputs = tf.repeat(new_gating_outputs,tf.shape(self.gating_kernel)[0]*tf.shape(self.gating_kernel)[1]*tf.shape(self.gating_kernel)[2],axis=3)\n",
    "        new_gating_outputs=tf.reshape(new_gating_outputs,(tf.shape(new_gating_outputs)[0],tf.shape(new_gating_outputs)[1],tf.shape(new_gating_outputs)[2],tf.shape(self.gating_kernel)[0],tf.shape(self.gating_kernel)[1],tf.shape(self.gating_kernel)[2]))\n",
    "        new_gating_outputs=tf.transpose(new_gating_outputs,perm=(0,1,3,2,4,5))\n",
    "        new_gating_outputs=tf.reshape(new_gating_outputs,(tf.shape(new_gating_outputs)[0],tf.shape(new_gating_outputs)[1]*tf.shape(new_gating_outputs)[2],tf.shape(new_gating_outputs)[3]*tf.shape(new_gating_outputs)[4],tf.shape(new_gating_outputs)[5]))\n",
    "        outputs = inputs*new_gating_outputs\n",
    "        return outputs, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SERp0GMl5Qkh"
   },
   "source": [
    "# Wideresnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750841615839,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "qMoWs8YN5Qkh"
   },
   "outputs": [],
   "source": [
    "initializer_gate=keras.initializers.RandomNormal(mean=0.0,stddev=0.0001)\n",
    "\n",
    "def WideResnetBlock(x, channels, strides, channel_mismatch=False):\n",
    "\n",
    "    identity = x\n",
    "\n",
    "    out = layers.BatchNormalization()(x)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=strides, padding='same')(out)\n",
    "\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.Conv2D(filters=channels, kernel_size=3, strides=1, padding='same')(out)\n",
    "\n",
    "    if channel_mismatch is not False:\n",
    "        identity = layers.Conv2D(filters=channels, kernel_size=1, strides=strides, padding='valid')(identity)\n",
    "\n",
    "    out = layers.Add()([identity, out])\n",
    "\n",
    "    return out\n",
    "\n",
    "def WideResnetGroup(x, num_blocks, channels, strides):\n",
    "\n",
    "    x = WideResnetBlock(x=x, channels=channels, strides=strides, channel_mismatch=True)\n",
    "\n",
    "    for _ in range(num_blocks - 1):\n",
    "        x = WideResnetBlock(x=x, channels=channels, strides=(1, 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "def WideResnet(x, num_blocks, k, num_classes=10):\n",
    "    widths = [int(v * k) for v in (16, 32, 64)]\n",
    "\n",
    "    x = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = WideResnetGroup(x, num_blocks, widths[0], strides=(1, 1))\n",
    "    x = WideResnetGroup(x, num_blocks, widths[1], strides=(2, 2))\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=640, kernel_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    x_1, indices_1 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_2, indices_2 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_3, indices_3 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "    x_4, indices_4 = gate(16,(1,1),(1,1),gating_activation=tf.nn.softmax,gating_kernel_initializer=initializer_gate)(x)\n",
    "\n",
    "    x_1 = layers.BatchNormalization()(x_1)\n",
    "    x_2 = layers.BatchNormalization()(x_2)\n",
    "    x_3 = layers.BatchNormalization()(x_3)\n",
    "    x_4 = layers.BatchNormalization()(x_4)\n",
    "\n",
    "    x_1 = layers.ReLU()(x_1)\n",
    "    x_2 = layers.ReLU()(x_2)\n",
    "    x_3 = layers.ReLU()(x_3)\n",
    "    x_4 = layers.ReLU()(x_4)\n",
    "\n",
    "    x_1 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_1)\n",
    "    x_2 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_2)\n",
    "    x_3 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_3)\n",
    "    x_4 = layers.Conv2D(filters=160, kernel_size=1, strides=1, padding='same')(x_4)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([x_1, x_2, x_3, x_4])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.AveragePooling2D((8,8))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-d9cWBl5Qki"
   },
   "source": [
    "# Trigger generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1750841615876,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "3ySd1qQa5Qki"
   },
   "outputs": [],
   "source": [
    "class GenerateSQRTrigger:\n",
    "    \"\"\"\n",
    "    A class that creates a random square pattern that is used as a trigger for an\n",
    "    image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, pos_label, dataset='mnist'):\n",
    "\n",
    "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
    "                               \"cifar10\": (32, 32, 3),\n",
    "                               \"fmnist\": (28, 28, 1)}\n",
    "\n",
    "        dims = datasets_dimensions[dataset]\n",
    "\n",
    "        if size[0] != size[1]:\n",
    "            raise Exception(\"The size of the trigger must be square.\")\n",
    "\n",
    "        if pos_label.lower() not in [\"upper-left\", \"upper-mid\", \"upper-right\", \"mid-left\", \"mid-mid\", \"mid-right\",\n",
    "                                     \"lower-left\",\n",
    "                                     \"lower-mid\", \"lower-right\"]:\n",
    "            raise Exception(\n",
    "                \"The position of the trigger must be one of the following: upper-left, upper-mid, upper-right, mid-left, mid-mid, mid-right, lower-left, lower-mid, lower-right\")\n",
    "\n",
    "        if size[0] > dims[0] or size[1] > dims[1]:\n",
    "            raise Exception(\"The size of the trigger is too large for the dataset items.\")\n",
    "\n",
    "        self.dims = dims\n",
    "        self.size = size\n",
    "        self.pos_label = pos_label\n",
    "        self.pos_coords = self._gen_pos_square()\n",
    "\n",
    "        trigger = np.zeros(self.dims, dtype=np.float32)\n",
    "        self.crafted_trigger = self.create_trigger_square(trigger)\n",
    "\n",
    "    def _gen_pos_square(self):\n",
    "        if self.pos_label == \"upper-left\":\n",
    "            return (0, 0)\n",
    "        elif self.pos_label == \"upper-mid\":\n",
    "            return (0, self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"upper-right\":\n",
    "            return (0, self.dims[1] - self.size[1])\n",
    "\n",
    "        elif self.pos_label == \"mid-left\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2, 0)\n",
    "        elif self.pos_label == \"mid-mid\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2,\n",
    "                    self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"mid-right\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2, self.dims[1] - self.size[1])\n",
    "\n",
    "        elif self.pos_label == \"lower-left\":\n",
    "            return (self.dims[0] - self.size[0], 0)\n",
    "        elif self.pos_label == \"lower-mid\":\n",
    "            return (self.dims[0] - self.size[0], self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"lower-right\":\n",
    "            return (self.dims[0] - self.size[0], self.dims[1] - self.size[1])\n",
    "\n",
    "    def create_trigger_square(self, trigger):\n",
    "        \"\"\"Create a square trigger.\"\"\"\n",
    "        base_x, base_y = self.pos_coords\n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                trigger[base_x + x][base_y + y] = \\\n",
    "                    np.ones((self.dims[2]))\n",
    "\n",
    "        return trigger\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "\n",
    "        base_x, base_y = self.pos_coords\n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                img[base_x + x][base_y + y] = self.crafted_trigger[base_x + x][base_y + y]\n",
    "        return img\n",
    "\n",
    "class GenerateBlendedTrigger:\n",
    "    \"\"\"\n",
    "    A class that uses images of the same dimensions as the dataset as triggers\n",
    "    that will be blended with the clean images.\n",
    "\n",
    "    We will use a random pattern or a hello-kitty image as the original paper\n",
    "    (https://arxiv.org/pdf/1712.05526.pdf).\n",
    "    \"\"\"\n",
    "    hello_kitty_path = \"/home/jtelintelo/pmoe_backdoor/hello_kitty.jpg\"\n",
    "\n",
    "    def __init__(self, dataset, trigger):\n",
    "\n",
    "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
    "                               \"cifar10\": (32, 32, 3),\n",
    "                               \"fmnist\": (28, 28, 1)}\n",
    "\n",
    "        dims = datasets_dimensions[dataset]\n",
    "\n",
    "        if trigger not in [\"random\", \"hello-kitty\"]:\n",
    "            raise Exception(f\"Pick 'random' or 'hello-kitty' trigger\")\n",
    "\n",
    "        if dataset not in datasets_dimensions:\n",
    "            raise Exception(f\"Dataset is not supported\")\n",
    "\n",
    "        self.dims = dims\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Generate the correct trigger\n",
    "        self.crafted_trigger = self.trigger_blended(trigger)\n",
    "\n",
    "    def trigger_blended(self, trigger):\n",
    "        \"\"\"Prepare the trigger for blended attack.\"\"\"\n",
    "        if trigger == \"hello-kitty\":\n",
    "            # Load kitty\n",
    "            img = Image.open(self.hello_kitty_path)\n",
    "\n",
    "            # Resize to dimensions\n",
    "            tmp = img.resize(self.dims[:-1])\n",
    "\n",
    "            if self.dims[2] == 1:\n",
    "                tmp = ImageOps.grayscale(tmp)\n",
    "\n",
    "            tmp = np.asarray(tmp)\n",
    "            # This is needed in case the image is grayscale (width x height) to\n",
    "            # add the channel dimension\n",
    "            tmp = tmp.reshape((self.dims))\n",
    "            # print(type(tmp))\n",
    "            # print(tmp.shape)\n",
    "            if patch_level:\n",
    "              pil_image = Image.fromarray(tmp)\n",
    "              resized_pil = pil_image.resize((8,8))\n",
    "              tmp = np.array(resized_pil)\n",
    "            # print(type(tmp))\n",
    "            # print(tmp.shape)\n",
    "            # plt.figure(figsize=(3, 3))\n",
    "            # plt.imshow((tmp), cmap='gray')\n",
    "            # plt.axis('off')\n",
    "            # plt.title(\"blended patch|\")\n",
    "            # plt.show()\n",
    "            # dd\n",
    "            # print(tmp.shape)\n",
    "            trigger_array = tmp / 255\n",
    "        else:\n",
    "            # Create a np.array with the correct dimensions\n",
    "            # fill the pixels with random values\n",
    "            trigger_array = (np.random.random((self.dims)))\n",
    "\n",
    "        return trigger_array\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "        crafted_trigger_normalized = self.crafted_trigger\n",
    "        if crafted_trigger_normalized.max() > 1:\n",
    "            crafted_trigger_normalized = crafted_trigger_normalized / 255.0\n",
    "        # Ensure the input image is normalized to [0, 1]\n",
    "        if img.max() > 1:\n",
    "            img = img / 255.0\n",
    "\n",
    "        # Blend the images\n",
    "        img = ((img*weight) + (crafted_trigger_normalized*(1-weight)))\n",
    "        # plt.figure(figsize=(4, 4))\n",
    "        # plt.imshow(img, cmap='gray' if self.dims[-1] == 1 else None)\n",
    "        # plt.axis('off')\n",
    "        # plt.title(\"Crafted Trigger\")\n",
    "        # plt.show()\n",
    "        # print(f\"Blended image min: {img.min()}, max: {img.max()}\")\n",
    "        return img.astype(np.float32)\n",
    "\n",
    "class GenerateWarpedTrigger:\n",
    "    \"\"\"\n",
    "    A class that generates a warped trigger using a distortion grid for backdoor attacks.\n",
    "    Compatible with TensorFlow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, s=_warping, grid_rescale=1.0, k = 2, input_height = 32):\n",
    "        \"\"\"\n",
    "        Initialize the warped trigger generator.\n",
    "        :param dataset: Dataset name (e.g., 'mnist', 'cifar10', etc.) for defining image dimensions.\n",
    "        :param s: Strength of the warping effect.\n",
    "        :param grid_rescale: Rescaling factor for the distortion grid.\n",
    "        \"\"\"\n",
    "        datasets_dimensions = {\"mnist\": (28, 28, 1),\n",
    "                               \"cifar10\": (32, 32, 3),\n",
    "                               \"fmnist\": (28, 28, 1)}\n",
    "\n",
    "        if dataset not in datasets_dimensions:\n",
    "            raise Exception(f\"Dataset is not supported\")\n",
    "\n",
    "        self.dims = datasets_dimensions[dataset]\n",
    "        self.s = s\n",
    "        self.k = k\n",
    "        self.input_height = input_height\n",
    "        self.grid_rescale = grid_rescale\n",
    "\n",
    "        # Initialize the identity grid and noise grid for warping\n",
    "        self.identity_grid, self.noise_grid = self.generate_main_grid()\n",
    "\n",
    "    def generate_main_grid(self):\n",
    "        \"\"\"\n",
    "        Generate the identity and noise grids for the warped trigger.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create coarse random noise grid\n",
    "        grid_noise = tf.random.uniform(\n",
    "            shape=(1, self.k, self.k, 2), minval=-1.0, maxval=1.0\n",
    "        )\n",
    "        grid_noise = grid_noise / tf.reduce_mean(tf.abs(grid_noise))\n",
    "\n",
    "        # Upsample the coarse noise to match the input height and width\n",
    "        noise_grid = tf.image.resize(grid_noise, size=(self.input_height, self.input_height), method=\"bicubic\")\n",
    "        noise_grid = tf.clip_by_value(noise_grid, -1.0, 1.0)  # Clamp values for stability\n",
    "\n",
    "        # Create the identity grid\n",
    "        array1d = tf.linspace(-1.0, 1.0, self.input_height)\n",
    "        x, y = tf.meshgrid(array1d, array1d)\n",
    "        identity_grid = tf.stack([y, x], axis=-1)\n",
    "        identity_grid = identity_grid[tf.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "        return identity_grid, noise_grid\n",
    "\n",
    "    def _grid_sample(self, image, grid):\n",
    "        \"\"\"\n",
    "        TensorFlow implementation of grid sampling for image warping.\n",
    "        :param image: The input image tensor with shape (batch_size, height, width, channels).\n",
    "        :param grid: The grid tensor with shape (batch_size, height, width, 2).\n",
    "        :return: Warped image tensor.\n",
    "        \"\"\"\n",
    "        batch_size, height, width, channels = image.shape\n",
    "\n",
    "        # Split grid into x and y components\n",
    "        grid_y, grid_x = tf.split(grid, 2, axis=-1)\n",
    "\n",
    "        # Rescale normalized grid coordinates to image pixel indices\n",
    "        grid_x = tf.cast((grid_x + 1.0) * 0.5 * tf.cast(width - 1, tf.float32), tf.int32)\n",
    "        grid_y = tf.cast((grid_y + 1.0) * 0.5 * tf.cast(height - 1, tf.float32), tf.int32)\n",
    "\n",
    "        # Remove the last dimension of grid_x and grid_y to match batch_indices shape\n",
    "        grid_x = tf.squeeze(grid_x, axis=-1)  # Shape: (batch_size, height, width)\n",
    "        grid_y = tf.squeeze(grid_y, axis=-1)  # Shape: (batch_size, height, width)\n",
    "\n",
    "        # Create batch indices for gather_nd\n",
    "        batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]  # Shape: (batch_size, 1, 1)\n",
    "        batch_indices = tf.tile(batch_indices, [1, height, width])  # Shape: (batch_size, height, width)\n",
    "\n",
    "        # Clip grid indices to stay within image bounds\n",
    "        grid_x = tf.clip_by_value(grid_x, 0, width - 1)\n",
    "        grid_y = tf.clip_by_value(grid_y, 0, height - 1)\n",
    "\n",
    "        # Stack indices for gather_nd\n",
    "        indices = tf.stack([batch_indices, grid_y, grid_x], axis=-1)\n",
    "\n",
    "        sampled_image = tf.gather_nd(image, indices)\n",
    "\n",
    "        return sampled_image\n",
    "\n",
    "    def poison(self, image):\n",
    "        \"\"\"\n",
    "        Apply a warping trigger to the image.\n",
    "        :param image: A NumPy array representing the input image.\n",
    "        :return: A NumPy array of the warped image.\n",
    "        \"\"\"\n",
    "        # Ensure the input image is normalized\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Expand dimensions to (batch_size, height, width, channels)\n",
    "        image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "        if len(image_tensor.shape) == 3:  # Add batch dimension if missing\n",
    "            image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "        # Generate the warped grid\n",
    "        grid_temps = (self.identity_grid + self.s * self.noise_grid / self.input_height) * self.grid_rescale\n",
    "        grid_temps = tf.clip_by_value(grid_temps, -1.0, 1.0)\n",
    "\n",
    "        # Warp the image using TensorFlow's grid_sample equivalent\n",
    "        poisoned_image = self._grid_sample(image_tensor, grid_temps)\n",
    "\n",
    "        # Squeeze batch dimension and convert back to NumPy\n",
    "        poisoned_image = tf.squeeze(poisoned_image, axis=0).numpy()\n",
    "\n",
    "        return poisoned_image\n",
    "\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"\n",
    "        Alias for the poison function for consistency with other trigger generators.\n",
    "        :param img: Input image as a NumPy array.\n",
    "        :return: Warped image as a NumPy array.\n",
    "        \"\"\"\n",
    "        return self.poison(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFNe3RD19qe3"
   },
   "source": [
    "# Creating backdoor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750841615896,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "BBMzxM7Y5Qki"
   },
   "outputs": [],
   "source": [
    "class BackdoorDataset:\n",
    "  \"\"\"\n",
    "  TensorFlow-compatible dataset for backdoor attacks, enabling poisoning of specific samples.\n",
    "  \"\"\"\n",
    "  def __init__(self, clean_data, clean_labels, trigger_obj, epsilon=0.08,\n",
    "                target_label=None, source_label=None, train=True, cifar=True):\n",
    "    \"\"\"\n",
    "    Initialize the backdoor dataset.\n",
    "    :param clean_data: Original dataset images (NumPy array).\n",
    "    :param clean_labels: Original dataset labels (one-hot encoded NumPy array).\n",
    "    :param trigger_obj: Instance of the GenerateSQRTrigger class.\n",
    "    :param epsilon: Fraction of samples to poison (default: 0.08 or 8%).\n",
    "    :param target_label: The target label for poisoned samples.\n",
    "    :param source_label: The source label for poisoned samples.\n",
    "    :param train: Whether this dataset is for training or testing.\n",
    "    \"\"\"\n",
    "    self.clean_data = clean_data\n",
    "    self.clean_labels = clean_labels\n",
    "    self.trigger_obj = trigger_obj\n",
    "    self.epsilon = epsilon\n",
    "    self.target_label = target_label\n",
    "    self.source_label = source_label\n",
    "    self.train = train\n",
    "    self.cifar = cifar\n",
    "\n",
    "    if train:\n",
    "      self.poisoned_data, self.poisoned_labels = self.get_train_set()\n",
    "    else:\n",
    "      self.poisoned_data, self.poisoned_labels = self.get_test_set()\n",
    "\n",
    "  def poison(self, img):\n",
    "    \"\"\"Poison an image by applying the trigger.\"\"\"\n",
    "    if patch_level:\n",
    "      emp = EMPatches()\n",
    "      img_patches, indices = emp.extract_patches(img, patchsize=8, overlap=0)\n",
    "      for index, patch in enumerate(img_patches):\n",
    "        img_patches[index] = self.trigger_obj.apply_trigger(patch)\n",
    "      poisoned_img = emp.merge_patches(img_patches, indices)\n",
    "      # plt.figure(figsize=(3, 3))\n",
    "      # plt.imshow(poisoned_img)\n",
    "      # plt.axis('off')\n",
    "      # plt.title(\"Crafted Trigger\")\n",
    "      # plt.show()\n",
    "    else:\n",
    "      poisoned_img = self.trigger_obj.apply_trigger(img)\n",
    "    return poisoned_img\n",
    "\n",
    "  def get_train_set(self):\n",
    "    \"\"\"Generate the poisoned training set.\"\"\"\n",
    "    poisoned_data = np.copy(self.clean_data)\n",
    "    if (isinstance(self.trigger_obj, GenerateBlendedTrigger) and self.trigger_obj.crafted_trigger is not None) or \\\n",
    "    isinstance(self.trigger_obj, GenerateWarpedTrigger):\n",
    "      poisoned_data = poisoned_data / 255  # Apply normalization\n",
    "\n",
    "    poisoned_labels = np.copy(self.clean_labels)\n",
    "\n",
    "    num_samples = self.clean_data.shape[0]\n",
    "    num_poisoned = int(self.epsilon * num_samples)\n",
    "    poisoned_indices = np.random.choice(num_samples, size=num_poisoned, replace=False)\n",
    "\n",
    "    for idx in poisoned_indices:\n",
    "      label_idx = np.argmax(self.clean_labels[idx])  # Convert one-hot label to scalar\n",
    "\n",
    "      if self.source_label is not None:\n",
    "        if label_idx == self.source_label:\n",
    "          # Poison data and change the label to target label\n",
    "          poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
    "          if self.cifar is True:\n",
    "            poisoned_labels[idx] = tf.one_hot(self.target_label, depth=10).numpy()\n",
    "          else:\n",
    "            poisoned_labels[idx] = tf.one_hot(self.target_label, depth=43).numpy()\n",
    "        else:\n",
    "          # Poison data but keep the original label\n",
    "          poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
    "          # Label remains unchanged\n",
    "      else:\n",
    "        # Poison data and always change the label to target label\n",
    "        poisoned_data[idx] = self.poison(self.clean_data[idx])\n",
    "        if self.cifar is True:\n",
    "          poisoned_labels[idx] = tf.one_hot(self.target_label, depth=10).numpy()\n",
    "        else:\n",
    "          poisoned_labels[idx] = tf.one_hot(self.target_label, depth=43).numpy()\n",
    "\n",
    "    return poisoned_data, poisoned_labels\n",
    "\n",
    "  def get_test_set(self):\n",
    "    \"\"\"Generate the poisoned test set.\"\"\"\n",
    "    temp = deepcopy(self.clean_data)\n",
    "    poisoned_data = []\n",
    "    poisoned_labels = []\n",
    "\n",
    "    for idx in range(self.clean_data.shape[0]):\n",
    "      label_idx = np.argmax(self.clean_labels[idx])  # Convert one-hot label to scalar\n",
    "      if label_idx != self.target_label:\n",
    "        poisoned_data.append(self.poison(temp[idx]))\n",
    "        poisoned_labels.append(self.clean_labels[idx])\n",
    "    return np.array(poisoned_data), np.array(poisoned_labels)\n",
    "\n",
    "  def get_data(self):\n",
    "    \"\"\"Return the poisoned dataset.\"\"\"\n",
    "    return self.poisoned_data, self.poisoned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgbO-SZ59qe4"
   },
   "source": [
    "# Attack evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750841615903,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "ci5hdeSn5Qkj"
   },
   "outputs": [],
   "source": [
    "def find_source_indices(labels, source_label):\n",
    "    \"\"\"\n",
    "    Find indices of samples with the source label.\n",
    "    \"\"\"\n",
    "    indices = np.where(labels == source_label)[0]\n",
    "    return indices\n",
    "\n",
    "def find_non_source_indices(labels, source_label, target_label):\n",
    "    \"\"\"\n",
    "    Find indices of samples which do not have the source or target label.\n",
    "    \"\"\"\n",
    "    indices = np.where((labels != source_label) & (labels != target_label))[0]\n",
    "    return indices\n",
    "\n",
    "def count_non_source_misclassifications(original_labels, predicted_labels, source_label, target_label):\n",
    "    \"\"\"\n",
    "    Count misclassifications for non-source and non-target label samples.\n",
    "    \"\"\"\n",
    "    sub_non_source_total = 0\n",
    "    sub_misclassifications = 0\n",
    "\n",
    "    indices = find_non_source_indices(original_labels, source_label, target_label)\n",
    "    sub_non_source_total += len(indices)\n",
    "\n",
    "    for index in indices:\n",
    "        if predicted_labels[index] == target_label:\n",
    "            sub_misclassifications += 1\n",
    "    return sub_misclassifications, sub_non_source_total\n",
    "\n",
    "def count_source_specific_classifications(original_labels, predicted_labels, source_label, target_label):\n",
    "    \"\"\"\n",
    "    Count correct classifications for source label samples to target label.\n",
    "    \"\"\"\n",
    "    sub_total = 0\n",
    "    sub_correct = 0\n",
    "\n",
    "    indices = find_source_indices(original_labels, source_label)\n",
    "    sub_total += len(indices)\n",
    "\n",
    "    for index in indices:\n",
    "        if predicted_labels[index] == target_label:\n",
    "            sub_correct += 1\n",
    "    return sub_correct, sub_total\n",
    "\n",
    "def calculate_ASR(model, test_data, test_labels, target_label, source_label=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the Attack Success Rate (ASR) of the backdoored model.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    non_source_total = 0\n",
    "    misclassifications = 0\n",
    "\n",
    "    # Get model predictions\n",
    "    predictions = model.predict(test_data, batch_size=128)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    original_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    if source_label is not None:\n",
    "        # Source-specific attack\n",
    "        sub_correct, sub_total = count_source_specific_classifications(original_labels, predicted_labels, source_label, target_label)\n",
    "        correct += sub_correct\n",
    "        total += sub_total\n",
    "\n",
    "        if verbose:\n",
    "            sub_misclassifications, sub_non_source_total = count_non_source_misclassifications(original_labels, predicted_labels, source_label, target_label)\n",
    "            misclassifications += sub_misclassifications\n",
    "            non_source_total += sub_non_source_total\n",
    "    else:\n",
    "        # Source-agnostic attack\n",
    "        for i in range(len(original_labels)):\n",
    "            if original_labels[i] != target_label:\n",
    "                total += 1\n",
    "                # print(\"original: \", original_labels[i], \"predict: \", predicted_labels[i])\n",
    "                if predicted_labels[i] == target_label:\n",
    "                    correct += 1\n",
    "\n",
    "\n",
    "    attack_acc = (correct * 100.0) / total\n",
    "    print(f\"Attack accuracy: {round(attack_acc, 2)}%\")\n",
    "\n",
    "    if source_label and verbose:\n",
    "        print(f\"Misclassifications: {misclassifications}\")\n",
    "        print(f\"Non-source total: {non_source_total}\")\n",
    "        misclassification_rate = (misclassifications * 100.0) / non_source_total\n",
    "        print(f\"False Positive Rate: {round(misclassification_rate, 2)}%\")\n",
    "\n",
    "    return attack_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FoMExvQ5Qkj"
   },
   "source": [
    "# Backdoor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1750841616407,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "A_o7JimEGKvC",
    "outputId": "c7d4c3c3-d162-4d4a-af2b-f05d1f748a5b"
   },
   "outputs": [],
   "source": [
    "# testing_data = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/cifar_10_test_data_sorted.npy')\n",
    "# testing_label = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/cifar_10_test_label_sorted.npy')\n",
    "# image_patch = testing_data[5536:5537]\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# plt.imshow((image_patch[0]), cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Crafted Trigger\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1750841616685,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "E8cJ6g_EEPPh",
    "outputId": "ce1cb74c-8767-4a05-f6e3-64bd233dfc89"
   },
   "outputs": [],
   "source": [
    "# trigger_generator = GenerateWarpedTrigger(\"cifar10\", input_height = 8 if patch_level else 32)\n",
    "# patch_level = True\n",
    "# backdoor_test_dataset = BackdoorDataset(\n",
    "#   clean_data=testing_data[5536:5537],\n",
    "#   clean_labels=tf.one_hot(testing_label[5536:5537], depth=10).numpy(),\n",
    "#   trigger_obj=trigger_generator,  # Adjust this as needed for the trigger type\n",
    "#   epsilon=poisoning_rate,  # Apply the same poisoning rate as training\n",
    "#   target_label=target_label,\n",
    "#   source_label=source_label,\n",
    "#   train=False  # Specify that this is for testing\n",
    "# )\n",
    "# poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "# poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "# image_patch = poisoned_testing_data\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# plt.imshow((image_patch[0]), cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Crafted Trigger\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1750841616869,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "p2rSZxcdE1Im",
    "outputId": "5f79e563-ec74-4925-9a8b-e1f75304a1f8"
   },
   "outputs": [],
   "source": [
    "# trigger_generator = GenerateBlendedTrigger(\"cifar10\", \"hello-kitty\")\n",
    "# patch_level = True\n",
    "# backdoor_test_dataset = BackdoorDataset(\n",
    "#   clean_data=testing_data[5536:5537],\n",
    "#   clean_labels=tf.one_hot(testing_label[5536:5537], depth=10).numpy(),\n",
    "#   trigger_obj=trigger_generator,  # Adjust this as needed for the trigger type\n",
    "#   epsilon=poisoning_rate,  # Apply the same poisoning rate as training\n",
    "#   target_label=target_label,\n",
    "#   source_label=source_label,\n",
    "#   train=False  # Specify that this is for testing\n",
    "# )\n",
    "# poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "# poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "# image_patch = poisoned_testing_data\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# plt.imshow((image_patch[0]), cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Crafted Trigger\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "error",
     "timestamp": 1750841616967,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "dv3_kyYz5Qkj",
    "outputId": "71e2786f-690f-45b0-ddd3-3a36ebce6a61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 as dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751391405.988850  679051 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38484 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751391412.787566  680244 service.cc:152] XLA service 0x14d01c002d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751391412.787596  680244 service.cc:160]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-07-01 19:36:52.991617: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1751391413.585523  680244 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-07-01 19:36:54.612269: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-07-01 19:36:55.005524: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 4700 bytes spill stores, 4772 bytes spill loads\n",
      "\n",
      "2025-07-01 19:36:55.190677: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 6016 bytes spill stores, 5864 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  8/391\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - categorical_accuracy: 0.1523 - loss: 2.2520   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751391424.606598  680244 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m388/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - categorical_accuracy: 0.4004 - loss: 1.6453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:37:12.227811: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:12.446352: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:12.730028: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5052', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:12.815104: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 4700 bytes spill stores, 4772 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:12.953111: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5054', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:13.046897: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 6108 bytes spill stores, 6064 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:13.549646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4495', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.4011 - loss: 1.6435"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:37:22.662676: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_621', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:22.680039: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_621', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:22.864640: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_621', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:22.937771: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_621', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:23.263030: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_621', 1960 bytes spill stores, 2060 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:23.709705: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_621', 6108 bytes spill stores, 6064 bytes spill loads\n",
      "\n",
      "2025-07-01 19:37:26.065583: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.44 = (f32[1000,160,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[1000,160,32,32]{3,2,1,0} %bitcast.2191, f32[160,160,3,3]{3,2,1,0} %bitcast.2198, f32[160]{0} %bitcast.2200), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/home/jtelintelo/.local/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-07-01 19:37:26.080931: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.015421733s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.44 = (f32[1000,160,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[1000,160,32,32]{3,2,1,0} %bitcast.2191, f32[160,160,3,3]{3,2,1,0} %bitcast.2198, f32[160]{0} %bitcast.2200), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/home/jtelintelo/.local/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - categorical_accuracy: 0.2650 - loss: 2.0230\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 1: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 70ms/step - categorical_accuracy: 0.4013 - loss: 1.6430\n",
      "Epoch 2/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.5344 - loss: 1.3171\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 2: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.6327 - loss: 1.0473\n",
      "Epoch 3/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.5152 - loss: 1.4002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 3: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.7108 - loss: 0.8280\n",
      "Epoch 4/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.6622 - loss: 0.9844\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 4: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.7654 - loss: 0.6825\n",
      "Epoch 5/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7400 - loss: 0.7320\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 5: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.8087 - loss: 0.5666\n",
      "Epoch 6/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.6347 - loss: 1.0554\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 6: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.8396 - loss: 0.4783\n",
      "Epoch 7/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - categorical_accuracy: 0.7911 - loss: 0.5911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 7: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.8738 - loss: 0.3850\n",
      "Epoch 8/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.6876 - loss: 0.9582\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 8: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.8995 - loss: 0.3119\n",
      "Epoch 9/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7515 - loss: 0.7215\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 9: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.9283 - loss: 0.2401\n",
      "Epoch 10/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7185 - loss: 0.9261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 10: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.9534 - loss: 0.1738\n",
      "Epoch 11/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7169 - loss: 0.9403\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 11: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.9752 - loss: 0.1081\n",
      "Epoch 12/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7374 - loss: 0.8883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 12: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.9914 - loss: 0.0610\n",
      "Epoch 13/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.7550 - loss: 0.8298\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 13: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 0.9974 - loss: 0.0318\n",
      "Epoch 14/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - categorical_accuracy: 0.7981 - loss: 0.6800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 14: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - categorical_accuracy: 0.9996 - loss: 0.0150\n",
      "Epoch 15/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8236 - loss: 0.5702\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 15: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0082\n",
      "Epoch 16/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8250 - loss: 0.5789\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 16: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0058\n",
      "Epoch 17/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8226 - loss: 0.5900\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 17: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0048\n",
      "Epoch 18/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8320 - loss: 0.5689\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 18: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 19/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8271 - loss: 0.5884\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 19: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 20/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8293 - loss: 0.5782\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 20: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 21/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8224 - loss: 0.6194\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 21: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 22/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8272 - loss: 0.5850\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 22: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 23/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8249 - loss: 0.5997\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 23: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 24/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8267 - loss: 0.5971\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 24: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 25/25\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - categorical_accuracy: 0.8274 - loss: 0.5984\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Epoch 25: Captured patch assignments.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - categorical_accuracy: 1.0000 - loss: 0.0019\n",
      "\u001b[1m65/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:40:56.213159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2482', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-01 19:40:56.427000: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2482', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-01 19:40:56.771422: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2482', 1960 bytes spill stores, 2060 bytes spill loads\n",
      "\n",
      "2025-07-01 19:40:57.149385: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2482', 6108 bytes spill stores, 6064 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 65ms/step\n",
      "Attack accuracy: 97.26%\n",
      "Attack Success Rate (ASR) for s=50000: 97.25555555555556%\n"
     ]
    }
   ],
   "source": [
    "if trigger == 'square':\n",
    "  trigger_generator = GenerateSQRTrigger((_size, _size), 'upper-left')\n",
    "if trigger == 'blend':\n",
    "  trigger_generator = GenerateBlendedTrigger(\"cifar10\", \"hello-kitty\")\n",
    "if trigger == 'warped':\n",
    "  trigger_generator = GenerateWarpedTrigger(\"cifar10\", input_height = 8 if patch_level else 32)\n",
    "\n",
    "if dataset == 'CIFAR-10':\n",
    "  print(\"CIFAR-10 as dataset\")\n",
    "  training_data_all = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/cifar_10_train_data_sorted.npy')\n",
    "  training_label_all = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/cifar_10_train_label_sorted.npy')\n",
    "  testing_data = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/cifar_10_test_data_sorted.npy')\n",
    "  testing_label = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/cifar_10_test_label_sorted.npy')\n",
    "\n",
    "if dataset == 'GTSRB':\n",
    "  print(\"GTSRB as dataset\")\n",
    "  training_data = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/gtsrb_train_data_sorted.npy')\n",
    "  training_label = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/gtsrb_train_label_sorted.npy')\n",
    "  testing_data = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/gtsrb_test_data_sorted.npy')\n",
    "  testing_label = np.load('/home/jtelintelo/pmoe_backdoor/backdoor_datasets/gtsrb_test_label_sorted.npy')\n",
    "\n",
    "# if train:\n",
    "#     it = 1\n",
    "# else:\n",
    "it = 50000\n",
    "for s in [it]:\n",
    "  # Loading the Data\n",
    "  if dataset == 'CIFAR-10':\n",
    "    # Sampling training data\n",
    "    training_data = np.concatenate((training_data_all[0:0+(s//10)], training_data_all[5000:5000+(s//10)],\n",
    "                                    training_data_all[10000:10000+(s//10)], training_data_all[15000:15000+(s//10)],\n",
    "                                    training_data_all[20000:20000+(s//10)], training_data_all[25000:25000+(s//10)],\n",
    "                                    training_data_all[30000:30000+(s//10)], training_data_all[35000:35000+(s//10)],\n",
    "                                    training_data_all[40000:40000+(s//10)], training_data_all[45000:45000+(s//10)]), axis=0)\n",
    "    training_label = np.concatenate((training_label_all[0:0+(s//10)], training_label_all[5000:5000+(s//10)],\n",
    "                                     training_label_all[10000:10000+(s//10)], training_label_all[15000:15000+(s//10)],\n",
    "                                     training_label_all[20000:20000+(s//10)], training_label_all[25000:25000+(s//10)],\n",
    "                                     training_label_all[30000:30000+(s//10)], training_label_all[35000:35000+(s//10)],\n",
    "                                     training_label_all[40000:40000+(s//10)], training_label_all[45000:45000+(s//10)]), axis=0)\n",
    "\n",
    "    # training_data = poison_dataset(training_data, poison_rate, trigger_generator)\n",
    "    backdoor_training_dataset = BackdoorDataset(\n",
    "      clean_data=training_data,\n",
    "      clean_labels=tf.one_hot(training_label, depth=10).numpy(),\n",
    "      trigger_obj=trigger_generator,\n",
    "      epsilon=poisoning_rate,\n",
    "      target_label=target_label,\n",
    "      source_label=source_label,\n",
    "      train=True\n",
    "    )\n",
    "    poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
    "\n",
    "    backdoor_test_dataset = BackdoorDataset(\n",
    "      clean_data=testing_data,\n",
    "      clean_labels=tf.one_hot(testing_label, depth=10).numpy(),\n",
    "      trigger_obj=trigger_generator,  # Adjust this as needed for the trigger type\n",
    "      epsilon=poisoning_rate,  # Apply the same poisoning rate as training\n",
    "      target_label=target_label,\n",
    "      source_label=source_label,\n",
    "      train=False  # Specify that this is for testing\n",
    "    )\n",
    "    poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "\n",
    "    # 1-of-K encoding\n",
    "    training_label = tf.reshape(tf.one_hot(training_label, axis=1, depth=10, dtype=tf.float64), (s, 10)).numpy()\n",
    "    testing_label = tf.reshape(tf.one_hot(testing_label, axis=1, depth=10, dtype=tf.float64), (10000, 10)).numpy()\n",
    "\n",
    "    # Shuffling the training set\n",
    "    indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
    "    training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
    "    poisoned_training_data = tf.gather(poisoned_training_data, shuffled_indices, axis=0)\n",
    "    poisoned_training_label = tf.gather(poisoned_training_label, shuffled_indices, axis=0)\n",
    "\n",
    "    # Normalizing and reshaping data\n",
    "    if isinstance(backdoor_training_dataset.trigger_obj, GenerateSQRTrigger): # or isinstance(backdoor_training_dataset.trigger_obj, GenerateWarpedTrigger):  # Check if the trigger is the square trigger\n",
    "      poisoned_training_data = poisoned_training_data / 255\n",
    "      poisoned_testing_data = poisoned_testing_data / 255\n",
    "\n",
    "    training_data=training_data/255\n",
    "    training_data=tf.cast(training_data,dtype=tf.dtypes.float32)\n",
    "    poisoned_training_data = tf.cast(poisoned_training_data, dtype=tf.dtypes.float32)\n",
    "    poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "    testing_data = testing_data / 255\n",
    "    testing_data = tf.cast(testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "  if dataset == 'GTSRB':\n",
    "    backdoor_training_dataset = BackdoorDataset(\n",
    "      clean_data=training_data,\n",
    "      clean_labels=tf.one_hot(training_label, depth=43).numpy(),\n",
    "      trigger_obj=trigger_generator,\n",
    "      epsilon=poisoning_rate,\n",
    "      target_label=target_label,\n",
    "      source_label=source_label,\n",
    "      train=True,\n",
    "      cifar=False\n",
    "    )\n",
    "    poisoned_training_data, poisoned_training_label = backdoor_training_dataset.get_data()\n",
    "\n",
    "    backdoor_test_dataset = BackdoorDataset(\n",
    "      clean_data=testing_data,\n",
    "      clean_labels=tf.one_hot(testing_label, depth=43).numpy(),\n",
    "      trigger_obj=trigger_generator,  # Adjust this as needed for the trigger type\n",
    "      epsilon=poisoning_rate,  # Apply the same poisoning rate as training\n",
    "      target_label=target_label,\n",
    "      source_label=source_label,\n",
    "      train=False,  # Specify that this is for testing\n",
    "      cifar=False\n",
    "    )\n",
    "    poisoned_testing_data, poisoned_testing_label = backdoor_test_dataset.get_data()\n",
    "\n",
    "    training_label = tf.reshape(tf.one_hot(training_label, depth=43, axis=1, dtype=tf.float64), (len(training_label), 43)).numpy()\n",
    "    testing_label = tf.reshape(tf.one_hot(testing_label, depth=43, axis=1, dtype=tf.float64), (len(testing_label), 43)).numpy()\n",
    "\n",
    "    indices = tf.range(start=0, limit=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    training_data = tf.gather(training_data, shuffled_indices, axis=0)\n",
    "    training_label = tf.gather(training_label, shuffled_indices, axis=0)\n",
    "    poisoned_training_data = tf.gather(poisoned_training_data, shuffled_indices, axis=0)\n",
    "    poisoned_training_label = tf.gather(poisoned_training_label, shuffled_indices, axis=0)\n",
    "\n",
    "    if isinstance(backdoor_training_dataset.trigger_obj, GenerateSQRTrigger): # or isinstance(backdoor_training_dataset.trigger_obj, GenerateWarpedTrigger):  # Check if the trigger is the square trigger\n",
    "      poisoned_training_data = poisoned_training_data / 255\n",
    "      poisoned_testing_data = poisoned_testing_data / 255\n",
    "\n",
    "    training_data=training_data/255\n",
    "    training_data=tf.cast(training_data,dtype=tf.dtypes.float32)\n",
    "    poisoned_training_data = tf.cast(poisoned_training_data, dtype=tf.dtypes.float32)\n",
    "    poisoned_testing_data = tf.cast(poisoned_testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "    testing_data = testing_data / 255\n",
    "    testing_data = tf.cast(testing_data, dtype=tf.dtypes.float32)\n",
    "\n",
    "  _index = 4536 # dog\n",
    "  # _index = 217 # 80 sign\n",
    "  # for _index in [80,111,130,139,157,169,178]:\n",
    "      # print(_index)\n",
    "  image_patch = poisoned_testing_data[_index:_index+1]\n",
    "  plt.figure(figsize=(3, 3))\n",
    "  plt.imshow((image_patch[0]), cmap='gray')\n",
    "  plt.axis('off')\n",
    "  # plt.title(f\"{trigger.title()}\", size=30, pad = 20)\n",
    "  # plt.savefig(f'/home/jtelintelo/pmoe_backdoor/result_images/{trigger}_patchlevel-{patch_level}.eps', format='eps', bbox_inches='tight')\n",
    "  plt.savefig(f'/home/jtelintelo/pmoe_backdoor/result_images/{dataset}_{trigger}_patchlevel-{patch_level}.pdf', bbox_inches='tight')\n",
    "  plt.show()\n",
    "\n",
    "  # Creating the model\n",
    "  model_input = tf.keras.Input(shape=(poisoned_training_data.shape[1], poisoned_training_data.shape[2], poisoned_training_data.shape[3]))\n",
    "  if dataset == 'CIFAR-10':\n",
    "    model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=10)\n",
    "  if dataset == 'GTSRB':\n",
    "    model_output = WideResnet(model_input, num_blocks=1, k=10, num_classes=43)\n",
    "\n",
    "  # Model Aggregation\n",
    "  model = tf.keras.Model(model_input, model_output)\n",
    "  # Model Compilation\n",
    "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['categorical_accuracy'])\n",
    "\n",
    "  # Callbacks\n",
    "  z = []\n",
    "  weights_dict = {}\n",
    "  patch_assignments = []\n",
    "  def capture_patch_assignments(epoch, logs):\n",
    "    try:\n",
    "      intermediate_model = tf.keras.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=[layer.output[1] for layer in model.layers if isinstance(layer, gate)]\n",
    "      )\n",
    "      # print(\"Intermediate model outputs:\", intermediate_model.outputs)\n",
    "\n",
    "      patch_indices = intermediate_model.predict(image_patch, batch_size=128)\n",
    "      patch_indices = np.array(patch_indices)\n",
    "      epoch_assignments = []  # To store assignments for this epoch\n",
    "      for expert_idx, indices in enumerate(patch_indices):\n",
    "        # Map indices to experts\n",
    "        flattened_indices = indices.flatten()\n",
    "        grid_coordinates = [(i // 8, i % 8) for i in flattened_indices]  # Convert to (row, col)\n",
    "        epoch_assignments.append({\n",
    "          \"expert\": expert_idx + 1,\n",
    "          \"flat_indices\": flattened_indices,\n",
    "          \"grid_coordinates\": grid_coordinates\n",
    "        })\n",
    "\n",
    "      patch_assignments.append(epoch_assignments)\n",
    "      print(f\"Epoch {epoch+1}: Captured patch assignments.\")\n",
    "\n",
    "    except Exception as e:\n",
    "      import traceback\n",
    "      print(f\"Error capturing patch assignments at epoch {epoch+1}: {e}\")\n",
    "      traceback.print_exc()\n",
    "\n",
    "  assignment_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=capture_patch_assignments)\n",
    "  weight_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: weights_dict.update({epoch: model.get_weights()}))\n",
    "\n",
    "  testing_after_epoch = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: z.append(model.evaluate(testing_data, testing_label, batch_size=1000, verbose=1)))\n",
    "\n",
    "\n",
    "  if load_model:\n",
    "    model.load_weights(f'/home/jtelintelo/pmoe_backdoor/model_files/{dataset}_{trigger}_{poisoning_rate}-poisonrate_{n_experts}-experts_patchlevel-{patch_level}.weights.h5')\n",
    "  # Train the Model\n",
    "  if train:  \n",
    "    x = model.fit(poisoned_training_data, poisoned_training_label, batch_size=128, epochs=25,\n",
    "                callbacks=[testing_after_epoch, weight_callback, assignment_callback])\n",
    "    f = f'/home/jtelintelo/pmoe_backdoor/results/test_acc_loss_{dataset}_{trigger}_{poisoning_rate}-poisonrate_{n_experts}-experts_patchlevel-{patch_level}' + str(s // 1000)\n",
    "    np.save(f, z)\n",
    "\n",
    "  asr = calculate_ASR(\n",
    "      model=model,\n",
    "      test_data=poisoned_testing_data,\n",
    "      test_labels=poisoned_testing_label,\n",
    "      target_label=target_label,\n",
    "      source_label=source_label,  # None if source-agnostic\n",
    "      verbose=True\n",
    "  )\n",
    "  print(f\"Attack Success Rate (ASR) for s={s}: {asr}%\")\n",
    "\n",
    "  if save_model:\n",
    "    model.save_weights(f'/home/jtelintelo/pmoe_backdoor/model_files/{dataset}_{trigger}_{poisoning_rate}-poisonrate_{n_experts}-experts_patchlevel-{patch_level}.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _index = 5536 # this is the dog image in non poisoned test data\n",
    "# for _index in [217]:\n",
    "# # for _index, image in enumerate(testing_data):\n",
    "#     print(_index)\n",
    "#     image_patch = poisoned_testing_data[_index:_index+1]\n",
    "#     plt.figure(figsize=(3, 3))\n",
    "#     plt.imshow(image_patch[0], cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     # plt.title(\"Clean\", size=30, pad = 20)\n",
    "#     # plt.savefig(f'/home/jtelintelo/pmoe_backdoor/result_images/{trigger}_patchlevel-{patch_level}.eps', format='eps')\n",
    "#     # plt.savefig(f'/home/jtelintelo/pmoe_backdoor/result_images/{trigger}_patchlevel-{patch_level}.pdf')\n",
    "#     plt.savefig(f'/home/jtelintelo/pmoe_backdoor/result_images/clean_{dataset}.pdf', bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "aborted",
     "timestamp": 1750841617034,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "MR-SzQKVEMbR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample point 50000: /home/jtelintelo/pmoe_backdoor/results/test_acc_loss_CIFAR-10_square_0.02-poisonrate_4-experts_patchlevel-False50\n",
      "Looking for file: /home/jtelintelo/pmoe_backdoor/results/test_acc_loss_CIFAR-10_square_0.02-poisonrate_4-experts_patchlevel-False50.npy\n",
      "Loaded file: /home/jtelintelo/pmoe_backdoor/results/test_acc_loss_CIFAR-10_square_0.02-poisonrate_4-experts_patchlevel-False50.npy\n",
      "Average Accuracy and Loss (last epoch):\n",
      "[[5.84286153e-01 8.35799992e-01 5.00000000e+04]]\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Attack accuracy: 97.26%\n",
      "Attack Success Rate (ASR) for s=50000: 97.25555555555556%\n"
     ]
    }
   ],
   "source": [
    "def avg_stddev_calc(f, no_v):\n",
    "  t_1 = None\n",
    "  for i in range(no_v):\n",
    "    # f_t = f + '_v' + str(i + 1) + '.npy'  # Construct the file name\n",
    "    f_t = f + '.npy'  # Construct the file name\n",
    "    print(f\"Looking for file: {f_t}\")  # Debugging: Print the file path\n",
    "\n",
    "    try:\n",
    "      t = np.load(f_t)  # Attempt to load the file\n",
    "      print(f\"Loaded file: {f_t}\")  # Confirm the file was loaded successfully\n",
    "      t = tf.reshape(t, (1, tf.shape(t)[0], tf.shape(t)[1])).numpy()\n",
    "\n",
    "      if t_1 is None:\n",
    "        t_1 = t\n",
    "      else:\n",
    "        t_1 = tf.concat((t_1, t), axis=0).numpy()\n",
    "    except FileNotFoundError:\n",
    "      print(f\"File {f_t} not found. Skipping.\")\n",
    "\n",
    "  if t_1 is None:\n",
    "    raise ValueError(\"No valid files found for computation.\")\n",
    "\n",
    "  t_av = tf.math.reduce_mean(t_1, axis=0).numpy()\n",
    "  t_std = tf.math.reduce_std(t_1, axis=0).numpy()\n",
    "  return t_av, t_std\n",
    "\n",
    "\n",
    "def last_epoch_result_collection(f, no_sample_points, points, no_v=5, last_epoch=50):\n",
    "  t_av_s = np.zeros((no_sample_points, 3), dtype=np.float64)\n",
    "  t_std_s = np.zeros((no_sample_points, 3), dtype=np.float64)\n",
    "\n",
    "  for i in range(no_sample_points):\n",
    "    f_1 = f'/home/jtelintelo/pmoe_backdoor/results/test_acc_loss_{dataset}_{trigger}_{poisoning_rate}-poisonrate_{n_experts}-experts_patchlevel-{patch_level}' + str(s // 1000)  # Construct base filename for each sample point\n",
    "        \n",
    "    print(f\"Processing sample point {points[i]}: {f_1}\")  # Debugging: Print base file name\n",
    "\n",
    "    t_av, t_std = avg_stddev_calc(f_1, no_v)  # Get average and stddev\n",
    "\n",
    "    t_av_s[i, 0] = t_av[last_epoch - 1, 0]  # Accuracy at the last epoch\n",
    "    t_av_s[i, 1] = t_av[last_epoch - 1, 1]  # Loss at the last epoch\n",
    "    t_av_s[i, 2] = points[i]  # Sample size\n",
    "\n",
    "    t_std_s[i, 0] = t_std[last_epoch - 1, 0]  # Stddev of accuracy\n",
    "    t_std_s[i, 1] = t_std[last_epoch - 1, 1]  # Stddev of loss\n",
    "    t_std_s[i, 2] = points[i]  # Sample size\n",
    "\n",
    "  return t_av_s, t_std_s\n",
    "\n",
    "# Set file and sample information\n",
    "f_moe = 'test_acc_loss_cifar_10_no_noise_wideresnet_moe'\n",
    "no_sample_points = 1\n",
    "points = [50000]\n",
    "no_v = 1\n",
    "last_epoch = 25\n",
    "\n",
    "# Perform analysis\n",
    "try:\n",
    "  wideresnet_moe_av_s, wideresnet_moe_std_s = last_epoch_result_collection(f_moe, no_sample_points, points, no_v, last_epoch)\n",
    "  # Print results\n",
    "  print(\"Average Accuracy and Loss (last epoch):\")\n",
    "  print(wideresnet_moe_av_s)\n",
    "\n",
    "except ValueError as e:\n",
    "  print(f\"Error during analysis: {e}\")\n",
    "\n",
    "asr = calculate_ASR(\n",
    "    model=model,\n",
    "    test_data=poisoned_testing_data,\n",
    "    test_labels=poisoned_testing_label,\n",
    "    target_label=target_label,\n",
    "    source_label=source_label,  # None if source-agnostic\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"Attack Success Rate (ASR) for s={s}: {asr}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1750841617038,
     "user": {
      "displayName": "Jona te Lintelo",
      "userId": "10992960807295070461"
     },
     "user_tz": -120
    },
    "id": "bUJ8rR3kFxUI"
   },
   "outputs": [],
   "source": [
    "# class KerasPruning:\n",
    "#     def __init__(self, x_train, y_train, model, layer_names, prune_rate):\n",
    "#         \"\"\"\n",
    "#         Prunes specific layers in a Keras model.\n",
    "\n",
    "#         Args:\n",
    "#             model (tf.keras.Model): The trained Keras model.\n",
    "#             layer_names (list): List of names of layers to prune.\n",
    "#             prune_rate (float): Fraction of filters to remove.\n",
    "#             x_train (tf.data.Dataset): Dataset for computing activations.\n",
    "#             y_train (tf.data.Dataset): Corresponding labels for dataset.\n",
    "#         \"\"\"\n",
    "#         self.x_train = x_train\n",
    "#         self.y_train = y_train\n",
    "#         self.model = model\n",
    "#         self.layer_names = layer_names  # List of layer names to prune\n",
    "#         self.prune_rate = prune_rate\n",
    "\n",
    "#     def get_layer_activations(self, layer_name):\n",
    "#         \"\"\"\n",
    "#         Runs the dataset through the model and collects activations of the target layer.\n",
    "#         \"\"\"\n",
    "#         activation_model = tf.keras.Model(\n",
    "#             inputs=self.model.input,\n",
    "#             outputs=self.model.get_layer(layer_name).output\n",
    "#         )\n",
    "\n",
    "#         batch_size = 8\n",
    "#         activations = []\n",
    "#         for i in range(0, len(self.x_train), batch_size):\n",
    "#             batch = self.x_train[i:i+batch_size]\n",
    "#             batch_activations = activation_model(batch, training=False)\n",
    "#             activations.append(batch_activations)\n",
    "\n",
    "#         return tf.concat(activations, axis=0)  # Shape: (num_samples, H, W, C)\n",
    "\n",
    "#     def prune(self):\n",
    "#         \"\"\"\n",
    "#         Prunes filters in the selected layers based on their average activation.\n",
    "#         \"\"\"\n",
    "#         for layer_name in self.layer_names:\n",
    "#             print(f\"Pruning layer: {layer_name}\")\n",
    "\n",
    "#             # Get the layer\n",
    "#             layer = self.model.get_layer(layer_name)\n",
    "\n",
    "#             # Get activations for the layer\n",
    "#             activations = self.get_layer_activations(layer_name)\n",
    "#             mean_activations = tf.reduce_mean(activations, axis=[0, 1, 2])  # Shape: (C,)\n",
    "\n",
    "#             # Sort filters by activation\n",
    "#             num_filters = mean_activations.shape[0]\n",
    "#             num_pruned_filters = int(num_filters * self.prune_rate)\n",
    "#             sorted_indices = tf.argsort(mean_activations)[:num_pruned_filters]  # Least active filters\n",
    "\n",
    "#             # Get layer weights\n",
    "#             weights, biases = layer.get_weights()  # Weights shape: (H, W, C_in, C_out)\n",
    "\n",
    "#             # Set pruned filters to zero\n",
    "#             weights[:, :, :, sorted_indices.numpy()] = 0\n",
    "#             biases[sorted_indices.numpy()] = 0\n",
    "\n",
    "#             # Assign updated weights back to the layer\n",
    "#             layer.set_weights([weights, biases])\n",
    "\n",
    "#             print(f\"Pruned {num_pruned_filters}/{num_filters} filters in {layer_name}\")\n",
    "\n",
    "#         return self.model  # Return pruned model\n",
    "# def get_layer_by_index(model, layer_type, index):\n",
    "#     \"\"\"\n",
    "#     Retrieves the correct layer name dynamically based on its type and order.\n",
    "\n",
    "#     Args:\n",
    "#         model (tf.keras.Model): The trained model.\n",
    "#         layer_type (tf.keras.layers.Layer): The type of layer to search for (e.g., tf.keras.layers.Conv2D).\n",
    "#         index (int): The occurrence index of the layer (0-based).\n",
    "\n",
    "#     Returns:\n",
    "#         str: The dynamically assigned name of the layer.\n",
    "#     \"\"\"\n",
    "#     layers = [layer.name for layer in model.layers if isinstance(layer, layer_type)]\n",
    "\n",
    "#     if index >= len(layers):\n",
    "#         raise ValueError(f\"Model has only {len(layers)} layers of type {layer_type}, but index {index} was requested.\")\n",
    "\n",
    "#     return layers[index]  # Return the dynamic layer name\n",
    "# layer_names_to_prune = [\n",
    "#     get_layer_by_index(model, tf.keras.layers.Conv2D, 8),\n",
    "#     get_layer_by_index(model, tf.keras.layers.Conv2D, 9),\n",
    "#     get_layer_by_index(model, tf.keras.layers.Conv2D, 10),\n",
    "#     get_layer_by_index(model, tf.keras.layers.Conv2D, 11),\n",
    "# ]\n",
    "\n",
    "# test_loss, test_accuracy = model.evaluate(testing_data, testing_label, batch_size=1000)\n",
    "# print(test_loss, test_accuracy)\n",
    "\n",
    "# print(f\"Selected layers to prune: {layer_names_to_prune}\")\n",
    "# pruner = KerasPruning(x_train=training_data[:10000], y_train=training_label[:10000], model=model, layer_names=layer_names_to_prune, prune_rate=_pr)\n",
    "# pruned_model = pruner.prune()\n",
    "# attack_acc = calculate_ASR(\n",
    "#             model=pruned_model,\n",
    "#             test_data=poisoned_testing_data,\n",
    "#             test_labels=poisoned_testing_label,\n",
    "#             target_label=target_label,\n",
    "#             source_label=source_label,  # None if source-agnostic\n",
    "#             verbose=True\n",
    "#         )\n",
    "# print(f\"Before fine tuning. Attack Success Rate (ASR) for s={s}: {attack_acc}%\")\n",
    "# test_loss, test_accuracy = pruned_model.evaluate(testing_data, testing_label, batch_size=1000)\n",
    "# print(test_loss, test_accuracy)\n",
    "\n",
    "# pruned_model.fit(training_data,training_label,batch_size=128,epochs=5,callbacks=[testing_after_epoch, weight_callback, assignment_callback])\n",
    "# test_loss, test_accuracy = pruned_model.evaluate(testing_data, testing_label, batch_size=1000)\n",
    "# print(test_loss, test_accuracy)\n",
    "\n",
    "# if save_pruned_model:\n",
    "#  pruned_model.save_weights(f'/home/jtelintelo/pmoe_backdoor/model_files/pruned_{dataset}_{trigger}_{poisoning_rate}-poisonrate_{n_experts}-experts_patchlevel-{patch_level}.weights.h5')\n",
    "# if load_pruned_model:\n",
    "#  pruned_model.load_weights(f'/home/jtelintelo/pmoe_backdoor/model_files/pruned_{dataset}_{trigger}_{poisoning_rate}-poisonrate_{n_experts}-experts_patchlevel-{patch_level}.weights.h5')\n",
    "\n",
    "# attack_acc = calculate_ASR(\n",
    "#             model=pruned_model,\n",
    "#             test_data=poisoned_testing_data,\n",
    "#             test_labels=poisoned_testing_label,\n",
    "#             target_label=target_label,\n",
    "#             source_label=source_label,  # None if source-agnostic\n",
    "#             verbose=True\n",
    "#         )\n",
    "# print(f\"After fine tuning. Attack Success Rate (ASR) for s={s}: {attack_acc}%\")\n",
    "# test_loss, test_accuracy = pruned_model.evaluate(testing_data, testing_label, batch_size=1000)\n",
    "# print(test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index in range(4536,4537): #401,402 GTSRB #3108 cat cifar\n",
    "  patch_assignments2 = []\n",
    "  image_patch = poisoned_testing_data[index:index+1]\n",
    "  intermediate_model = tf.keras.Model(\n",
    "                      inputs=model.input,\n",
    "                      outputs=[layer.output[1] for layer in model.layers if isinstance(layer, gate)]\n",
    "                  )\n",
    "  patch_indices = intermediate_model.predict(image_patch, batch_size=128)\n",
    "  patch_indices = np.array(patch_indices)\n",
    "  epoch_assignments = []  # To store assignments for this epoch\n",
    "  for expert_idx, indices in enumerate(patch_indices):\n",
    "    flattened_indices = indices.flatten()\n",
    "    grid_coordinates = [(i // 8, i % 8) for i in flattened_indices]  # Convert to (row, col)\n",
    "    epoch_assignments.append({\n",
    "      \"expert\": expert_idx + 1,\n",
    "      \"flat_indices\": flattened_indices,\n",
    "      \"grid_coordinates\": grid_coordinates\n",
    "    })\n",
    "\n",
    "  patch_assignments2.append(epoch_assignments)\n",
    "\n",
    "  grid_tracking = {expert_id: np.zeros((8, 8), dtype=int) for expert_id in range(1, 5)}\n",
    "  # for epoch_assignments in [patch_assignments[-1]]:\n",
    "  for epoch_assignments in patch_assignments2:\n",
    "      for assignment in epoch_assignments:\n",
    "          expert_id = assignment[\"expert\"]\n",
    "          # print(expert_id)\n",
    "          # print(assignment)\n",
    "          for (row, col) in assignment[\"grid_coordinates\"]:\n",
    "              # print(row, col)\n",
    "              grid_tracking[expert_id][row, col] += 1\n",
    "\n",
    "  def visualize_expert_specialization(grid_tracking):\n",
    "      fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "      axes[0].imshow(image_patch[0], cmap='gray')\n",
    "      axes[0].axis('off')\n",
    "      for expert_id, grid in grid_tracking.items():\n",
    "          ax = axes[expert_id]\n",
    "          cax = ax.imshow(grid, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "          # ax.set_title(f\"Expert {expert_id}\", size=30, pad=15)\n",
    "          # ax.set_xlabel(\"Columns\")\n",
    "          # ax.set_ylabel(\"Rows\")\n",
    "          # fig.colorbar(cax, ax=ax)\n",
    "          ax.set_xticks([])\n",
    "          ax.set_yticks([])\n",
    "      plt.tight_layout()\n",
    "      plt.savefig(f'/home/jtelintelo/pmoe_backdoor/result_images/imagelevel_patch_selection_{trigger}.pdf', bbox_inches='tight')\n",
    "      plt.show()\n",
    "\n",
    "  visualize_expert_specialization(grid_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
